{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import cv2\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import gc\n",
    "import _pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the VGG16 Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Image classification with weights trained on ImageNet:\n",
    "Xception\n",
    "VGG16\n",
    "VGG19\n",
    "ResNet, ResNetV2\n",
    "InceptionV3\n",
    "InceptionResNetV2\n",
    "MobileNet\n",
    "MobileNetV2\n",
    "DenseNet\n",
    "NASNet\n",
    "\n",
    "https://keras.io/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[0m\u001b[01;34mdensenet201\u001b[0m/\r\n",
      "DensenetDeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34minception\u001b[0m/\r\n",
      "InceptionDeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34minceptionresnetv2\u001b[0m/\r\n",
      "InceptionResnetV2DeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34mmobilenet\u001b[0m/\r\n",
      "MobilenetDeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34mmobilenetv2\u001b[0m/\r\n",
      "MobilenetV2DeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34mresnet50\u001b[0m/\r\n",
      "Resnet50DeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34mvgg16\u001b[0m/\r\n",
      "VggDeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34mxception\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_name\t\t= 'vgg16'\n",
    "\n",
    "weights \t\t= 'imagenet'\n",
    "data_path \t\t= '../dataset/plates'\n",
    "features_path\t= './' + model_name + '/features.h5'\n",
    "labels_path \t= './' + model_name + '/labels.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "base_model = VGG16(weights=weights)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc1').output)\n",
    "image_size = (224, 224)\n",
    "\n",
    "load_model_time = time.time() - start\n",
    "print(\"[INFO] loading model time: {}\".format(load_model_time))\n",
    "print(\"[INFO] successfully loaded base model: {}\".format(model_name))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the labels from dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = os.listdir(data_path)\n",
    "print(train_labels)\n",
    "le = LabelEncoder()\n",
    "le.fit([tl for tl in train_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "features = []\n",
    "labels   = []\n",
    "\n",
    "i = 0\n",
    "for label in train_labels:\n",
    "\tcur_path = data_path + \"/\" + label\n",
    "\tfor image_path in glob.glob(cur_path + \"/*.jpg\"):   \n",
    "\t\timg = image.load_img(image_path, target_size=image_size)\n",
    "\t\tx = image.img_to_array(img)\n",
    "\t\tx = np.expand_dims(x, axis=0)\n",
    "\t\tx = preprocess_input(x)\n",
    "\t\tfeature = model.predict(x)\n",
    "\t\tflat = feature.flatten()\n",
    "\t\tfeatures.append(flat)\n",
    "\t\tlabels.append(label)\n",
    "\t\tif i % 100 == 0:  # print every 100th processed image\n",
    "\t\t\tprint(\"[INFO] processed - {}\".format(i))\n",
    "\t\ti += 1\n",
    "\tprint(\"[INFO] completed label - {}\".format(label))\n",
    "\n",
    "\n",
    "feature_extraction_time = time.time() - start\n",
    "print(\"[INFO] Feature extraction time: {}\".format(feature_extraction_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetNames = np.unique(labels)\n",
    "le = LabelEncoder()\n",
    "le_labels = le.fit_transform(labels)\n",
    "\n",
    "print (\"[STATUS] training labels: {}\".format(le_labels))\n",
    "print (\"[STATUS] training labels shape: {}\".format(le_labels.shape))\n",
    "\n",
    "h5f_data = h5py.File(features_path, 'w')\n",
    "array_of_features = np.array(features)\n",
    "h5f_data.create_dataset('dataset', data=array_of_features)\n",
    "\n",
    "print(\"[INFO] Feature max value: {}\".format(np.amax(array_of_features)))\n",
    "print(\"[INFO] Feature min value: {}\".format(np.amin(array_of_features)))\n",
    "\n",
    "h5f_label = h5py.File(labels_path, 'w')\n",
    "h5f_label.create_dataset('dataset', data=np.array(le_labels))\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "\n",
    "print (\"[STATUS] features and labels saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"[STATUS] end time - {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")))\n",
    "print(\" Loading model time: {}\".format(load_model_time))\n",
    "print(\" Feature extraction time: {}\".format(feature_extraction_time))\n",
    "print(\" Feature extraction time per sample: {}\".format(feature_extraction_time/len(le_labels)))\n",
    "gc.collect()  # garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size\t\t= 0.20\n",
    "results\t\t\t= './' + model_name + '/results.txt'\n",
    "classifier_path = './' + model_name + '/classifier.cpickle' \n",
    "seed \t\t    = 4444\n",
    "num_classes\t    = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     verticalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     verticalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f_data = h5py.File(features_path, 'r')\n",
    "h5f_label = h5py.File(labels_path, 'r')\n",
    "\n",
    "features_string = h5f_data['dataset']\n",
    "labels_string   = h5f_label['dataset']\n",
    "\n",
    "features = np.array(features_string)\n",
    "labels   = np.array(labels_string)\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "\n",
    "print (\"[INFO] Successfully loaded {} features.\".format(model_name))\n",
    "print (\"[INFO] features shape: {}\".format(features.shape))\n",
    "print (\"[INFO] labels shape: {}\".format(labels.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"[INFO] split into training and testing data...\")\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(np.array(features),\n",
    "                                                                  np.array(labels),\n",
    "                                                                  test_size=test_size,\n",
    "                                                                  random_state=seed)\n",
    "\n",
    "print (\"[INFO] splitted train and test data...\")\n",
    "print (\"[INFO] train data  : {}\".format(trainData.shape))\n",
    "print (\"[INFO] test data   : {}\".format(testData.shape))\n",
    "print (\"[INFO] train labels: {}\".format(trainLabels.shape))\n",
    "print (\"[INFO] test labels : {}\".format(testLabels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Classifier or Create a New One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # load classifier from file, ie. logistic regression\n",
    "    print(\"[INFO] loading classifier...\")\n",
    "    with open(classifier_path, 'rb') as fid:\n",
    "        classifier_model = _pickle.load(fid)\n",
    "except:\n",
    "    print(\"[INFO] creating model/training...\")\n",
    "    classifier_model = LogisticRegression(solver='lbfgs', random_state=seed,  multi_class='auto', max_iter=5000)\n",
    "    classifier_model.fit(trainData, trainLabels)\n",
    "\n",
    "    # Save the model\n",
    "    print(\"[INFO] saving classifier...\")\n",
    "    f = open(classifier_path, \"wb\")\n",
    "    f.write(_pickle.dumps(classifier_model))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(results, \"w\")\n",
    "rank_1 = 0\n",
    "\n",
    "# loop over test data\n",
    "start = time.time()\n",
    "for (label, features) in zip(testLabels, testData):\n",
    "\tpredictions = classifier_model.predict_proba(np.atleast_2d(features))[0]\n",
    "\tpredictions = np.argsort(predictions)[::-1][:5]\n",
    "\t# rank-1 prediction increment\n",
    "\tif label == predictions[0]:\n",
    "\t\trank_1 += 1\n",
    "\n",
    "# convert accuracies to percentages\n",
    "rank_1 = (rank_1 / float(len(testLabels))) * 100\n",
    "\n",
    "# write the accuracies to file\n",
    "f.write(\"rank-1: {}\\n\".format(rank_1))\n",
    "print(\"[INFO] rank 1 accuracy: {}\".format(rank_1))\n",
    "\n",
    "# evaluate the model of test data\n",
    "preds = classifier_model.predict(testData)\n",
    "\n",
    "# write the classification report to file\n",
    "print(classification_report(testLabels, preds))\n",
    "f.write(\"{}\\n\".format(classification_report(testLabels, preds)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(list(os.listdir(data_path)))\n",
    "title = 'Confusion matrix: ' + model_name\n",
    "target_names = sorted(list(os.listdir(data_path)))\n",
    "cm =confusion_matrix(testLabels, preds)\n",
    "plot_confusion_matrix(cm, target_names, title=title, cmap=None, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs  = classifier_model.predict_proba(testData)\n",
    "output = np.append(testLabels[:,None],probs, axis=1)\n",
    "df     = pd.DataFrame(output, columns = ['category'] + labels)\n",
    "sns.catplot(x=\"category\", y=\"1981\", data=df, kind=\"box\");   # kind=\"violin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"category\", y=\"2003\", data=df, kind=\"box\");   # kind=\"violin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"category\", y=\"2014\", data=df, kind=\"box\");   # kind=\"violin\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-mkc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
