{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import cv2\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import gc\n",
    "import _pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the MobilenetV2 Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Image classification with weights trained on ImageNet:\n",
    "Xception\n",
    "VGG16\n",
    "VGG19\n",
    "ResNet, ResNetV2\n",
    "InceptionV3\n",
    "InceptionResNetV2\n",
    "MobileNet\n",
    "MobileNetV2\n",
    "DenseNet\n",
    "NASNet\n",
    "\n",
    "https://keras.io/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir mobilenetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdataset\u001b[0m/\r\n",
      "DeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34mdensenet201\u001b[0m/\r\n",
      "DensenetDeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34minception\u001b[0m/\r\n",
      "InceptionDeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34minceptionresnetv2\u001b[0m/\r\n",
      "InceptionResnetV2DeepFeatureExtractionAndTraining.ipynb\r\n",
      "keras_model\r\n",
      "\u001b[01;35mkeras_model.png\u001b[0m\r\n",
      "\u001b[01;35mkeras_models.png\u001b[0m\r\n",
      "keras_models.png.1\r\n",
      "keras_models.png.2\r\n",
      "\u001b[01;34mmobilenet\u001b[0m/\r\n",
      "MobilenetDeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34mmobilenetv2\u001b[0m/\r\n",
      "MobilenetV2DeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34mresnet50\u001b[0m/\r\n",
      "Resnet50DeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34mvgg16\u001b[0m/\r\n",
      "VggDeepFeatureExtractionAndTraining.ipynb\r\n",
      "\u001b[01;34mxception\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_name\t\t= 'mobilenetv2'    # modify this depending on your model\n",
    "\n",
    "weights \t\t= 'imagenet'\n",
    "data_path \t\t= 'dataset/dogsvscats'\n",
    "features_path\t= './' + model_name + '/features.h5'\n",
    "labels_path \t= './' + model_name + '/labels.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "14540800/14536120 [==============================] - 29s 2us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No such layer: global_average_pooling2d_1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0365c7bbd4cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMobileNetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'global_average_pooling2d_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/keras-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m    537\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such layer: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No such layer: global_average_pooling2d_1"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "base_model = MobileNetV2(weights=weights)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('global_average_pooling2d_1').output)\n",
    "image_size = (224, 224)\n",
    "\n",
    "load_model_time = time.time() - start\n",
    "print(\"[INFO] loading model time: {}\".format(load_model_time))\n",
    "print(\"[INFO] successfully loaded base model: {}\".format(model_name))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the labels from dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'dogs']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = os.listdir(data_path)\n",
    "print(train_labels)\n",
    "le = LabelEncoder()\n",
    "le.fit([tl for tl in train_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed - 0\n",
      "[INFO] processed - 100\n",
      "[INFO] processed - 200\n",
      "[INFO] processed - 300\n",
      "[INFO] processed - 400\n",
      "[INFO] processed - 500\n",
      "[INFO] processed - 600\n",
      "[INFO] processed - 700\n",
      "[INFO] processed - 800\n",
      "[INFO] processed - 900\n",
      "[INFO] processed - 1000\n",
      "[INFO] processed - 1100\n",
      "[INFO] processed - 1200\n",
      "[INFO] processed - 1300\n",
      "[INFO] processed - 1400\n",
      "[INFO] processed - 1500\n",
      "[INFO] processed - 1600\n",
      "[INFO] processed - 1700\n",
      "[INFO] processed - 1800\n",
      "[INFO] processed - 1900\n",
      "[INFO] processed - 2000\n",
      "[INFO] processed - 2100\n",
      "[INFO] processed - 2200\n",
      "[INFO] processed - 2300\n",
      "[INFO] processed - 2400\n",
      "[INFO] processed - 2500\n",
      "[INFO] processed - 2600\n",
      "[INFO] processed - 2700\n",
      "[INFO] processed - 2800\n",
      "[INFO] processed - 2900\n",
      "[INFO] processed - 3000\n",
      "[INFO] processed - 3100\n",
      "[INFO] processed - 3200\n",
      "[INFO] processed - 3300\n",
      "[INFO] processed - 3400\n",
      "[INFO] processed - 3500\n",
      "[INFO] processed - 3600\n",
      "[INFO] processed - 3700\n",
      "[INFO] processed - 3800\n",
      "[INFO] processed - 3900\n",
      "[INFO] processed - 4000\n",
      "[INFO] processed - 4100\n",
      "[INFO] processed - 4200\n",
      "[INFO] processed - 4300\n",
      "[INFO] processed - 4400\n",
      "[INFO] processed - 4500\n",
      "[INFO] processed - 4600\n",
      "[INFO] processed - 4700\n",
      "[INFO] processed - 4800\n",
      "[INFO] processed - 4900\n",
      "[INFO] completed label - cats\n",
      "[INFO] processed - 5000\n",
      "[INFO] processed - 5100\n",
      "[INFO] processed - 5200\n",
      "[INFO] processed - 5300\n",
      "[INFO] processed - 5400\n",
      "[INFO] processed - 5500\n",
      "[INFO] processed - 5600\n",
      "[INFO] processed - 5700\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "features = []\n",
    "labels   = []\n",
    "\n",
    "i = 0\n",
    "for label in train_labels:\n",
    "\tcur_path = data_path + \"/\" + label\n",
    "\tfor image_path in glob.glob(cur_path + \"/*.jpg\"):   \n",
    "\t\timg = image.load_img(image_path, target_size=image_size)\n",
    "\t\tx = image.img_to_array(img)\n",
    "\t\tx = np.expand_dims(x, axis=0)\n",
    "\t\tx = preprocess_input(x)\n",
    "\t\tfeature = model.predict(x)\n",
    "\t\tflat = feature.flatten()\n",
    "\t\tfeatures.append(flat)\n",
    "\t\tlabels.append(label)\n",
    "\t\tif i % 100 == 0:  # print every 100th processed image\n",
    "\t\t\tprint(\"[INFO] processed - {}\".format(i))\n",
    "\t\ti += 1\n",
    "\tprint(\"[INFO] completed label - {}\".format(label))\n",
    "\n",
    "\n",
    "feature_extraction_time = time.time() - start\n",
    "print(\"[INFO] Feature extraction time: {}\".format(feature_extraction_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetNames = np.unique(labels)\n",
    "le = LabelEncoder()\n",
    "le_labels = le.fit_transform(labels)\n",
    "\n",
    "print (\"[STATUS] training labels: {}\".format(le_labels))\n",
    "print (\"[STATUS] training labels shape: {}\".format(le_labels.shape))\n",
    "\n",
    "h5f_data = h5py.File(features_path, 'w')\n",
    "array_of_features = np.array(features)\n",
    "h5f_data.create_dataset('dataset', data=array_of_features)\n",
    "\n",
    "print(\"[INFO] Feature max value: {}\".format(np.amax(array_of_features)))\n",
    "print(\"[INFO] Feature min value: {}\".format(np.amin(array_of_features)))\n",
    "\n",
    "h5f_label = h5py.File(labels_path, 'w')\n",
    "h5f_label.create_dataset('dataset', data=np.array(le_labels))\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "\n",
    "print (\"[STATUS] features and labels saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"[STATUS] end time - {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")))\n",
    "print(\" Loading model time: {}\".format(load_model_time))\n",
    "print(\" Feature extraction time: {}\".format(feature_extraction_time))\n",
    "print(\" Feature extraction time per sample: {}\".format(feature_extraction_time/len(le_labels)))\n",
    "gc.collect()  # garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size\t\t= 0.20\n",
    "results\t\t\t= './' + model_name + '/results.txt'\n",
    "classifier_path = './' + model_name + '/classifier.cpickle' \n",
    "seed \t\t    = 4444\n",
    "num_classes\t    = 2   # dogs, cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     verticalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     verticalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f_data = h5py.File(features_path, 'r')\n",
    "h5f_label = h5py.File(labels_path, 'r')\n",
    "\n",
    "features_string = h5f_data['dataset']\n",
    "labels_string   = h5f_label['dataset']\n",
    "\n",
    "features = np.array(features_string)\n",
    "labels   = np.array(labels_string)\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "\n",
    "print (\"[INFO] Successfully loaded {} features.\".format(model_name))\n",
    "print (\"[INFO] features shape: {}\".format(features.shape))\n",
    "print (\"[INFO] labels shape: {}\".format(labels.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"[INFO] split into training and testing data...\")\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(np.array(features),\n",
    "                                                                  np.array(labels),\n",
    "                                                                  test_size=test_size,\n",
    "                                                                  random_state=seed)\n",
    "\n",
    "print (\"[INFO] splitted train and test data...\")\n",
    "print (\"[INFO] train data  : {}\".format(trainData.shape))\n",
    "print (\"[INFO] test data   : {}\".format(testData.shape))\n",
    "print (\"[INFO] train labels: {}\".format(trainLabels.shape))\n",
    "print (\"[INFO] test labels : {}\".format(testLabels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Classifier or Create a New One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # load classifier from file, ie. logistic regression\n",
    "    print(\"[INFO] loading classifier...\")\n",
    "    with open(classifier_path, 'rb') as fid:\n",
    "        classifier_model = _pickle.load(fid)\n",
    "except:\n",
    "    print(\"[INFO] creating model/training...\")\n",
    "    classifier_model = LogisticRegression(solver='lbfgs', random_state=seed,  multi_class='auto', max_iter=5000)\n",
    "    classifier_model.fit(trainData, trainLabels)\n",
    "\n",
    "    # Save the model\n",
    "    print(\"[INFO] saving classifier...\")\n",
    "    f = open(classifier_path, \"wb\")\n",
    "    f.write(_pickle.dumps(classifier_model))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(results, \"w\")\n",
    "rank_1 = 0\n",
    "\n",
    "# loop over test data\n",
    "start = time.time()\n",
    "for (label, features) in zip(testLabels, testData):\n",
    "\tpredictions = classifier_model.predict_proba(np.atleast_2d(features))[0]\n",
    "\tpredictions = np.argsort(predictions)[::-1][:5]\n",
    "\t# rank-1 prediction increment\n",
    "\tif label == predictions[0]:\n",
    "\t\trank_1 += 1\n",
    "\n",
    "# convert accuracies to percentages\n",
    "rank_1 = (rank_1 / float(len(testLabels))) * 100\n",
    "\n",
    "# write the accuracies to file\n",
    "f.write(\"rank-1: {}\\n\".format(rank_1))\n",
    "print(\"[INFO] rank 1 accuracy: {}\".format(rank_1))\n",
    "\n",
    "# evaluate the model of test data\n",
    "preds = classifier_model.predict(testData)\n",
    "\n",
    "# write the classification report to file\n",
    "print(classification_report(testLabels, preds))\n",
    "f.write(\"{}\\n\".format(classification_report(testLabels, preds)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(list(os.listdir(data_path)))\n",
    "title = 'Confusion matrix: ' + model_name\n",
    "target_names = sorted(list(os.listdir(data_path)))\n",
    "cm =confusion_matrix(testLabels, preds)\n",
    "plot_confusion_matrix(cm, target_names, title=title, cmap=None, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs  = classifier_model.predict_proba(testData)\n",
    "output = np.append(testLabels[:,None],probs, axis=1)\n",
    "df     = pd.DataFrame(output, columns = ['category'] + labels)\n",
    "sns.catplot(x=\"category\", y=\"cats\", data=df, kind=\"box\");   # kind=\"violin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"category\", y=\"dogs\", data=df, kind=\"box\");   # kind=\"violin\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-mkc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
